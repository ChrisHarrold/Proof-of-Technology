{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark ML\n",
    "\n",
    "### In this notebook, we will explore machine learning using Spark ML. We will exploit Spark ML's high-level APIs built on top of DataFrames to create and tune machine learning pipelines. Spark ML Pipelines enable combining multiple algorithms into a single pipeline or workflow. We will heavily utilize Spark ML's feature transformers to convert, modify and scale the features that will be used to develop the machine learning model. Finally, we will evaluate and cross-validate our model to demonstrate the process of determining a best fit model and load the results in the database.\n",
    "\n",
    "### We will load generated travel data that has been examined for patterns of Human Trafficking from DashDB to do the machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Spark version and existence of Spark and Spark SQL contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spark version is 2.0.2.\n"
     ]
    }
   ],
   "source": [
    "print('The spark version is {}.'.format(spark.version))\n",
    "\n",
    "# No check for Spark SQL context (JEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need section on why these are needed (JEP)\n",
    "# discussion on supported libraries in DSX? (JEP)\n",
    "# are all these really needed? (JEP)\n",
    "\n",
    "# Imports for DashDB\n",
    "import jaydebeapi\n",
    "from ibmdbpy import IdaDataBase\n",
    "from ibmdbpy import IdaDataFrame\n",
    "\n",
    "# Imports for Spark\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make the connection to DashDB.  We need to do this slightly differently here - through JDBC to a Spark Context.  \n",
    "\n",
    "### These commands must be run once for the kernel, and the kernel must be restarted.  Once run, they can be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I did not run these and seemed to be OK (JEP)\n",
    "\n",
    "#!pip install jaydebeapi --user  \n",
    "#!pip install ibmdbpy --user \n",
    "#!pip install --user --upgrade pixiedust\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Why is this here and where did the jar come from? Still needed?  (JEP)\n",
    "\n",
    "import os\n",
    "os.environ['CLASSPATH'] = \"/usr/local/src/data-connectors-1.4.1/db2jcc4-10.5.0.6.jar\"\n",
    "import jpype\n",
    "args='-Djava.class.path=%s' % os.environ['CLASSPATH']\n",
    "jvm = jpype.getDefaultJVMPath()\n",
    "jpype.startJVM(jvm, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to add section on importing your own DashDB credentials (JEP)\n",
    "# Note: sometimes credentials come in as credentials_2, etc.   Change to credentials_1 if so (JEP)\n",
    "\n",
    "# @hidden_cell\n",
    "credentials_1 = {\n",
    "  'port':'50000',\n",
    "  'db':'BLUDB',\n",
    "  'username':'dash105370',\n",
    "  'ssljdbcurl':'jdbc:db2://awh-yp-small03.services.dal.bluemix.net:50001/BLUDB:sslConnection=true;',\n",
    "  'host':'awh-yp-small03.services.dal.bluemix.net',\n",
    "  'https_url':'https://awh-yp-small03.services.dal.bluemix.net:8443',\n",
    "  'dsn':'DATABASE=BLUDB;HOSTNAME=awh-yp-small03.services.dal.bluemix.net;PORT=50000;PROTOCOL=TCPIP;UID=dash105370;PWD=PEgQhDgLomgU;',\n",
    "  'hostname':'awh-yp-small03.services.dal.bluemix.net',\n",
    "  'jdbcurl':'jdbc:db2://awh-yp-small03.services.dal.bluemix.net:50000/BLUDB',\n",
    "  'ssldsn':'DATABASE=BLUDB;HOSTNAME=awh-yp-small03.services.dal.bluemix.net;PORT=50001;PROTOCOL=TCPIP;UID=dash105370;PWD=PEgQhDgLomgU;Security=SSL;',\n",
    "  'uri':'db2://dash105370:PEgQhDgLomgU@awh-yp-small03.services.dal.bluemix.net:50000/BLUDB',\n",
    "  'password':\"\"\"PEgQhDgLomgU\"\"\"\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection_string = 'jdbc:db2://{hostname}:{port}/{db}:user={username};password={password};'.format(**credentials_1)\n",
    "idadb = IdaDataBase(dsn=connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext=SQLContext(sc)\n",
    "trafficking_df = sqlContext.read.jdbc(credentials_1[\"jdbcurl\"],\n",
    "                                     'FEMALE_TRAFFICKING',\n",
    "                                      properties = {\"user\" : credentials_1[\"username\"], \"password\" : credentials_1[\"password\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the count and first several rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+------+---+----------+-------------+------------------+--------------------+--------------------+-----------+---------------+----------------+---------------------+--------------------+-----------------------+----------------------------+--------------------+----------------------------+----------------------+------------------------------+----------------------+------------------------------+------------------------+--------------------+\n",
      "|VETTING_LEVEL|               NAME|GENDER|AGE|BIRTH_DATE|BIRTH_COUNTRY|BIRTH_COUNTRY_CODE|          OCCUPATION|             ADDRESS|        SSN|PASSPORT_NUMBER|PASSPORT_COUNTRY|PASSPORT_COUNTRY_CODE|   COUNTRIES_VISITED|COUNTRIES_VISITED_COUNT|ARRIVAL_AIRPORT_COUNTRY_CODE|ARRIVAL_AIRPORT_IATA|ARRIVAL_AIRPORT_MUNICIPALITY|ARRIVAL_AIRPORT_REGION|DEPARTURE_AIRPORT_COUNTRY_CODE|DEPARTURE_AIRPORT_IATA|DEPARTURE_AIRPORT_MUNICIPALITY|DEPARTURE_AIRPORT_REGION|                UUID|\n",
      "+-------------+-------------------+------+---+----------+-------------+------------------+--------------------+--------------------+-----------+---------------+----------------+---------------------+--------------------+-----------------------+----------------------------+--------------------+----------------------------+----------------------+------------------------------+----------------------+------------------------------+------------------------+--------------------+\n",
      "|           30|Debb Suzanne Martin|     F| 46|1970-11-04|   Bangladesh|                BD|Land/geomatics su...|1751 Kaufman Moto...|740-65-1520|       32727544|      Bangladesh|                   BD|            OM,NZ,KW|                      3|                          US|                 EWR|                      Newark|                 US-NJ|                            OM|                   MCT|                        Muscat|                   OM-MA|a2ce710d-6505-416...|\n",
      "|          100| Aud Janice Freeman|     F| 22|1994-08-23|        Ghana|                GH|Investment banker...|0463 Miller Dale ...|034-89-4290|      701012638|           Ghana|                   GH|                  RU|                      1|                          US|                 FSM|                  Fort Smith|                 US-AR|                            RU|                   KJA|                   Krasnoyarsk|                  RU-KYA|288ba278-592d-441...|\n",
      "|          100|Wendy Anna Franklin|     F| 46|1970-10-02|        Ghana|                GH|Biochemist, clinical|34208 Rose Terrac...|387-46-0949|      183880173|           Ghana|                   GH|NZ,BA,BR,FI,UY,LB...|                      9|                          US|                 HMN|                  Alamogordo|                 US-NM|                            NZ|                   CHC|                  Christchurch|                  NZ-CAN|83d4d67d-e7b2-4d4...|\n",
      "|           30|       Latoya Scott|     F| 15|2001-09-02|        Ghana|                GH|Hydrographic surv...|26241 Olson Knl A...|623-95-9765|      928407599|           Ghana|                   GH|                  RU|                      1|                          US|                 SAV|                    Savannah|                 US-GA|                            RU|                   CKL|                        Moscow|                  RU-MOS|23378757-3d13-4ed...|\n",
      "|           30|  Kendr Jessi Watts|     F| 30|1987-01-11|       Brazil|                BR| Engineer, materials|986 Harris Lake, ...|414-08-6952|      367404380|          Brazil|                   BR|      LV,UY,ZA,RU,QA|                      5|                          US|                 AFW|                  Fort Worth|                 US-TX|                            LV|                   RIX|                          Riga|                  LV-RIX|ade7bddd-f0da-462...|\n",
      "+-------------+-------------------+------+---+----------+-------------+------------------+--------------------+--------------------+-----------+---------------+----------------+---------------------+--------------------+-----------------------+----------------------------+--------------------+----------------------------+----------------------+------------------------------+----------------------+------------------------------+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use pandas? Optional? (JEP)\n",
    "\n",
    "trafficking_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  We will use the 'VETTING_LEVEL' column as a label for training the machine learning model.  This is where our analyst has marked the data as vetted.  \n",
    "#### Spark ML requires that that the labels are data type Double, so we will cast the column as Double (it was inferred as Integer when read into Spark).\n",
    "#### withColumn() is a Spark SQL way to manipulate a dataframe.  Since an RDD is immutable, we need to create a new RDD each time we transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What exactly are the steps here? I think we need to explain more clearly.  (JEP)   \n",
    "DataWithLabels = (trafficking_df.withColumn(\"VettingTemp\", trafficking_df[\"VETTING_LEVEL\"]\n",
    "    .cast(\"Double\")).drop(\"VETTING_LEVEL\")\n",
    "    .withColumnRenamed(\"VettingTemp\", \"VETTING_LEVEL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to use year of birth intead of date of birth in our learning.  \n",
    "### Another way to transform an rdd in Spark is using SQL Syntax.  Here, we will be adding a new field, BIRTH_YEAR to our vetting set.  We will also just select the fields we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-----------------------+---------------------+------+----------+\n",
      "|                UUID|VETTING_LEVEL|                NAME|          OCCUPATION|COUNTRIES_VISITED_COUNT|PASSPORT_COUNTRY_CODE|GENDER|BIRTH_YEAR|\n",
      "+--------------------+-------------+--------------------+--------------------+-----------------------+---------------------+------+----------+\n",
      "|a2ce710d-6505-416...|         30.0| Debb Suzanne Martin|Land/geomatics su...|                      3|                   BD|     F|      1970|\n",
      "|288ba278-592d-441...|        100.0|  Aud Janice Freeman|Investment banker...|                      1|                   GH|     F|      1994|\n",
      "|83d4d67d-e7b2-4d4...|        100.0| Wendy Anna Franklin|Biochemist, clinical|                      9|                   GH|     F|      1970|\n",
      "|23378757-3d13-4ed...|         30.0|        Latoya Scott|Hydrographic surv...|                      1|                   GH|     F|      2001|\n",
      "|ade7bddd-f0da-462...|         30.0|   Kendr Jessi Watts| Engineer, materials|                      5|                   BR|     F|      1987|\n",
      "|b7af7d1e-4fb8-41d...|         30.0|     Tiffany Wheeler|Geneticist, molec...|                      3|                   GH|     F|      2001|\n",
      "|eeae268e-9a24-490...|        100.0|     Kayla Hernandez| Editorial assistant|                      2|                   GH|     F|      1987|\n",
      "|485740c5-314b-4a0...|         30.0|        Chris Guerra| Editorial assistant|                      3|                   GH|     F|      1997|\n",
      "|d6407a43-004a-4e5...|        100.0|       Morgan Hester|Chartered legal e...|                      4|                   GH|     F|      1993|\n",
      "|aeffe4f9-e31f-464...|        100.0|Bettye Amanda Valdez|      Science writer|                      2|                   GH|     F|      1993|\n",
      "|d9123832-4794-4be...|        100.0|        Missy Little|Advertising accou...|                      1|                   GH|     F|      1971|\n",
      "|7b0968aa-86b3-4ba...|        100.0| Anna Pamela Johnson|      Dramatherapist|                      1|                   GH|     F|      1973|\n",
      "|20014d21-faec-432...|         30.0|Robbi Sheila Mart...|Radiographer, the...|                     12|                   BR|     F|      1975|\n",
      "|e0d2618c-c146-40e...|         30.0|Tammie Pamela Bender|         Firefighter|                      4|                   GH|     F|      1997|\n",
      "|eca7ee57-b340-477...|        100.0| Robin Jessica Smith|Occupational ther...|                      1|                   GH|     F|      1983|\n",
      "|e35416bf-428f-421...|        100.0| Donna Deney Wilkins|Investment banker...|                      2|                   GH|     F|      1989|\n",
      "|7360a995-0a78-49d...|        100.0|      Kaitlin Harris|Clinical scientis...|                      2|                   GH|     F|      1989|\n",
      "|fe80386a-f008-496...|        100.0|Wanda Christee Gi...|    Industrial buyer|                      3|                   GH|     F|      1971|\n",
      "|2d3b027a-e516-4ce...|        100.0|Stephanie Chie Ga...|Therapist, hortic...|                      7|                   GH|     F|      1981|\n",
      "|dbc3d0f2-2446-4c8...|        100.0|     Cayley Gonzalez|Aeronautical engi...|                      1|                   GH|     F|      1984|\n",
      "+--------------------+-------------+--------------------+--------------------+-----------------------+---------------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DataWithLabels.createOrReplaceTempView(\"VettingData\")\n",
    "AllVettingData = sqlContext.sql (\"SELECT UUID, VETTING_LEVEL, NAME, OCCUPATION, COUNTRIES_VISITED_COUNT, PASSPORT_COUNTRY_CODE, GENDER, year(BIRTH_DATE) as BIRTH_YEAR FROM VettingData\")\n",
    "AllVettingData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that the majority of the data has not been labeled.  We can not use it for our training data, so filter it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabeledVettingData=AllVettingData.filter(\"VETTING_LEVEL != 100\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VETTING_LEVEL is in three different statuses:\n",
    "#### 10- HIGH\n",
    "#### 20- MEDIUM\n",
    "#### 30 - LOW\n",
    "\n",
    "\n",
    "### Print the total number of vetting statuses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows is 178.\n",
      "The number of rows labeled high is 42.\n",
      "The number of rows labeled medium is 40.\n",
      "The number of rows labeled low is 96.\n",
      "The number of unlabled rows is 0.\n"
     ]
    }
   ],
   "source": [
    "print('The total number of rows is {}.'.format(LabeledVettingData.count()))\n",
    "print('The number of rows labeled high is {}.'.format(LabeledVettingData.filter(LabeledVettingData['VETTING_LEVEL'] == 10).count()))\n",
    "print('The number of rows labeled medium is {}.'.format(LabeledVettingData.filter(LabeledVettingData['VETTING_LEVEL'] == 20).count()))\n",
    "print('The number of rows labeled low is {}.'.format(LabeledVettingData.filter(LabeledVettingData['VETTING_LEVEL'] == 30).count()))\n",
    "print('The number of unlabled rows is {}.'.format(LabeledVettingData.filter(LabeledVettingData['VETTING_LEVEL'] == 100).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of vetted records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabeledVettingData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even though our labels are already doubles, we want to index them so we can get string-based statuses based on our predictions.  For each vetting status, we need to convert each label to a double using StringIndexer().  The ML models will require the labels to be in a column called \"label\".\n",
    "\n",
    "Convert each label to a String?  (JEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"VETTING_LEVEL\", outputCol=\"label\", handleInvalid=\"error\")\n",
    "\n",
    "# We need to fit our data into a model in order to get a mapping of the double to the original vetting status later\n",
    "labelModel=labelIndexer.fit(LabeledVettingData)\n",
    "converter = IndexToString(inputCol=\"prediction\", outputCol=\"predCategory\", labels=labelModel.labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we need to index the data for machine learning.  SparkML has several indexers we can choose from. Each takes an input column and an output label that we will use in our pipeline later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringIndexer\n",
    "#### StringIndexer() is a transformer that encodes a string column to a column of indices. The indices are ordered by value frequencies, so the most frequent value gets index 0. If the input column is numeric, it is cast to string first.\n",
    "#### For our vetting dataset, we are interested in all string-based features so we will use the StringIndexer for them.\n",
    "#### We need to use 'handleInvalid=\"skip\"' because not all values have been validated in our vetting set.  That means the algorithms will skip these records.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occupationIndexer = StringIndexer(inputCol=\"OCCUPATION\", outputCol=\"occupationIndex\", handleInvalid=\"skip\")\n",
    "countryIndexer = StringIndexer(inputCol=\"PASSPORT_COUNTRY_CODE\", outputCol=\"countryIndex\", handleInvalid=\"skip\")\n",
    "genderIndexer = StringIndexer(inputCol=\"GENDER\", outputCol=\"genderIndex\", handleInvalid=\"skip\")\n",
    "yearOfBirthIndexer = StringIndexer(inputCol=\"BIRTH_YEAR\", outputCol=\"birthYearIndex\", handleInvalid=\"skip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A VectorAssembler puts all of the features into a simple array.  This combines all of our features into one.  COUNTRIIES_VISITED_COUNT is already a numeric, so we can just put that in the array as is.\n",
    "### The converter takes the double values of the predictions, and helps us convert them to our labels when we actually view them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# can we remove this next line?  (JEP)\n",
    "#vecAssembler = VectorAssembler(inputCols=[\"occupationIndex\",\"countryIndex\",\"departureCountryIndex\", \"birthdayCountryIndex\", \"genderIndex\", \"birthYearIndex\", \"COUNTRIES_VISITED_COUNT\"], outputCol=\"features\")\n",
    "converter = IndexToString(inputCol=\"prediction\", outputCol=\"predCategory\", labels=labelModel.labels)\n",
    "vecAssembler = VectorAssembler(inputCols=[\"occupationIndex\",\"countryIndex\",\"genderIndex\", \"birthYearIndex\", \"COUNTRIES_VISITED_COUNT\"], outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to unit norm\n",
    "### This normalization can help standardize your input data and improve the behavior of learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the model that we want to use\n",
    "\n",
    "### The model here is Decision Tree.  It will output each prediction into a 'prediction' column.  Decision Tree is a model that learns based on previous decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol=\"VETTING_LEVEL\", predictionCol=\"prediction\")\n",
    "\n",
    "# We say Decision Tree but are using Naive Bayes (JEP)\n",
    "#dt = DecisionTreeClassifier(maxDepth=5, labelCol=\"label\", maxBins=512)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Pipeline is a sequence of stages where each stage is either a Transformer or an Estimator\n",
    "### These stages are run in order and the input DataFrame is transformed as it passes through each stage.  The pipeline strings everything together.  First, comes the feature transformations, then the assembler to put them togather into one DataFrame.  We pass that into the model. \n",
    "\n",
    "### In machine learning, it is common to run a sequence of algorithms to process and learn from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer,occupationIndexer,countryIndexer, genderIndexer, yearOfBirthIndexer, vecAssembler, normalizer, nb, converter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, train the data\n",
    "### We split up the data randomly into 90% for training and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in the traininig data set is 159.\n",
      "The number of rows labeled high is 35.\n",
      "The number of rows labeled medium is 39.\n",
      "The number of rows labeled low is 85.\n",
      "\n",
      "The number of records in the test data set is 19.\n",
      "The number of rows labeled high is 7.\n",
      "The number of rows labeled medium is 1.\n",
      "The number of rows labeled low is 11.\n"
     ]
    }
   ],
   "source": [
    "train, test = LabeledVettingData.randomSplit([90.0,10.0], seed=1)\n",
    "train.cache()\n",
    "test.cache()\n",
    "print('The number of records in the traininig data set is {}.'.format(train.count()))\n",
    "print('The number of rows labeled high is {}.'.format(train.filter(train['VETTING_LEVEL'] == 10).count()))\n",
    "print('The number of rows labeled medium is {}.'.format(train.filter(train['VETTING_LEVEL'] == 20).count()))\n",
    "print('The number of rows labeled low is {}.'.format(train.filter(train['VETTING_LEVEL'] == 30).count()))\n",
    "print('')\n",
    "\n",
    "print('The number of records in the test data set is {}.'.format(test.count()))\n",
    "print('The number of rows labeled high is {}.'.format(test.filter(test['VETTING_LEVEL'] == 10).count()))\n",
    "print('The number of rows labeled medium is {}.'.format(test.filter(test['VETTING_LEVEL'] == 20).count()))\n",
    "print('The number of rows labeled low is {}.'.format(test.filter(test['VETTING_LEVEL'] == 30).count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the pipeline to the training data\n",
    "<div class=\"panel-group\" id=\"accordion-3\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse-3\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n",
    "          model = pipeline.fit(train)<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data assigning the result to a variable called 'model'.\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on document in the Test data set\n",
    "### Keep in mind that the model has not seen the data in the test data set\n",
    "<div class=\"panel-group\" id=\"accordion-4\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse-4\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-4\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n",
    "          predictions = model.transform(test)<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data assigning the result to a variable called 'predictions'.\n",
    "\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results\n",
    "### Note that we only got a small sample of the results back beacuse we have a very small amount of training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an evaluator for the binary classification using area under the ROC Curve as the evaluation metric\n",
    "\n",
    "### Receiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied\n",
    "\n",
    "The curve is created by plotting the true positive rate against the false positive rate at various threshold settings. The ROC curve is thus the sensitivity as a function of fall-out. The area under the ROC curve is useful for comparing and selecting the best machine learning model for a given data set. A model with an area under the ROC curve score near 1 has very good performance. A model with a score near 0.5 is about as good as flipping a coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve = 1.0.\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"VETTING_LEVEL\").setMetricName(\"areaUnderROC\")\n",
    "print('Area under the ROC curve = {}.'.format(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters\n",
    "### Generate hyperparameter combinations by taking the cross product of some parameter values\n",
    "\n",
    "Spark ML algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by Spark ML itself. Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder().addGrid(nb.smoothing, [0.25, 0.5, 0.75])\n",
    "                 .addGrid(normalizer.p, [1.0, 2.0])\n",
    "                 .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cross validator to tune the pipeline with the generated parameter grid\n",
    "Spark ML provides for cross-validation for hyperparameter tuning. Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one.  Note that since runs the model several times, it takes a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluate the ML Pipeline to find the best model\n",
    "### using the area under the ROC evaluator and hyperparameters specified in the parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve for best fitted model = 1.0.\n"
     ]
    }
   ],
   "source": [
    "cvModel = cv.fit(LabeledVettingData)\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(LabeledVettingData))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what improvement we achieve by tuning the hyperparameters using cross-evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve for non-tuned model = 1.0.\n",
      "Area under the ROC curve for best fitted model = 1.0.\n",
      "Improvement = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print('Area under the ROC curve for non-tuned model = {}.'.format(evaluator.evaluate(predictions)))\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(LabeledVettingData))))\n",
    "print('Improvement = {0:0.2f}%'.format((evaluator.evaluate(cvModel.transform(LabeledVettingData)) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We didn't do any better, so keep with the original model.  If it was better, we would go ahead and use \"cvModel\" instead of \"model\" below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we want to score the remaining records that were unscored, and load them into a new table in the database.\n",
    "### First, in order to run 'new' data through the machine algoriths, we need to remove the VETTING_LEVEL field from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[UUID: string, NAME: string, OCCUPATION: string, COUNTRIES_VISITED_COUNT: int, PASSPORT_COUNTRY_CODE: string, GENDER: string, BIRTH_YEAR: int]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember, the \"AllVettingData\" data frame has all the formatted data.\n",
    "AllVettingData.drop(\"VETTING_LEVEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the actual transformation on the unvetted data.\n",
    "<div class=\"panel-group\" id=\"accordion-3\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse-3\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n",
    "         newPreds = model.transform(AllVettingData)<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " newPreds = model.transform(AllVettingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the data we have predicted and some of the fields in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+--------------------+--------------------+------+-----------------------+---------------------+\n",
      "|                UUID|prediction|predCategory|         probability|                NAME|GENDER|COUNTRIES_VISITED_COUNT|PASSPORT_COUNTRY_CODE|\n",
      "+--------------------+----------+------------+--------------------+--------------------+------+-----------------------+---------------------+\n",
      "|8ec259f8-01a7-4dc...|       2.0|        20.0|[0.03189564023862...|       Sharie Harris|     F|                      4|                   GH|\n",
      "|29f51fdc-7a9d-4cf...|       2.0|        20.0|[0.00225631864189...|         Chrisy Byrd|     F|                      9|                   GH|\n",
      "|8e833de1-4491-4ed...|       1.0|        10.0|[0.05640117619351...|       Kayla Meadows|     F|                      1|                   GH|\n",
      "|8efaf3e0-b476-43c...|       2.0|        20.0|[0.02981222568762...|          Alyze Gray|     F|                      4|                   GH|\n",
      "|9d0f3b67-f869-453...|       0.0|        30.0|[0.48762807631001...|      Carlye Johnson|     F|                      7|                   GH|\n",
      "|1bb82ef8-46f7-4bf...|       0.0|        30.0|[0.59020458897001...|Tabby Moll Dominguez|     F|                     12|                   GH|\n",
      "|fc09787e-1275-435...|       1.0|        10.0|[0.04266846415299...|           Beki Hale|     F|                      4|                   GH|\n",
      "|bf593a30-c15b-45c...|       2.0|        20.0|[0.19233561141241...|       Cristie Moore|     F|                      4|                   GH|\n",
      "|f920719e-1def-4fc...|       2.0|        20.0|[3.29883356496625...|      Ami Washington|     F|                      5|                   GH|\n",
      "|766cc358-c850-4ba...|       1.0|        10.0|[0.01211034601777...|        Bonnie Smith|     F|                      3|                   GH|\n",
      "|3097505c-f0a1-439...|       1.0|        10.0|[0.00781568431978...|        Amber Montes|     F|                      2|                   GH|\n",
      "|c23f71eb-7e60-47b...|       1.0|        10.0|[0.04514863486865...|     Shannie Stevens|     F|                      3|                   GH|\n",
      "|a7a20754-826c-4e1...|       0.0|        30.0|[0.55894738133772...|Kris Tricia Williams|     F|                      8|                   GH|\n",
      "|e4212207-284b-4c6...|       2.0|        20.0|[0.00692863150440...| Brandi Denee Taylor|     F|                      5|                   BR|\n",
      "|d5564bc7-00d1-470...|       2.0|        20.0|[1.56866343878282...|        Bette Morris|     F|                      8|                   GH|\n",
      "|9a5ad373-271c-45d...|       0.0|        30.0|[0.70647397167365...|        Marye Palmer|     F|                      7|                   GH|\n",
      "|67cedbd3-7834-436...|       1.0|        10.0|[0.07514355990151...|        Alice Harris|     F|                      2|                   GH|\n",
      "|0150a7ca-4932-4fd...|       1.0|        10.0|[0.08611301793089...|      Samantha Moore|     F|                      6|                   BR|\n",
      "|ca350a07-f58b-429...|       2.0|        20.0|[5.05504817357388...|        Ashlee Hogan|     F|                      2|                   GH|\n",
      "|78dce4db-2550-4bc...|       2.0|        20.0|[0.00293353310522...| Rhonda Tammy Prince|     F|                      4|                   GH|\n",
      "+--------------------+----------+------------+--------------------+--------------------+------+-----------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newPreds.select(\"UUID\", \"prediction\", \"predCategory\", \"probability\", \"NAME\", \"GENDER\", \"COUNTRIES_VISITED_COUNT\", \"PASSPORT_COUNTRY_CODE\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember that VETTING_LEVEL is in three different statuses:\n",
    "#### 10- HIGH\n",
    "#### 20- MEDIUM\n",
    "#### 30 - LOW\n",
    "\n",
    "\n",
    "### Print the total number of vetting statuses that we predicted.  The actual predicted data is low because we only have a few vetted records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in the test data set is 321.\n",
      "The number of rows labeled high is 7.\n",
      "The number of rows labeled medium is 1.\n",
      "The number of rows labeled low is 11.\n"
     ]
    }
   ],
   "source": [
    "print('The number of records in the test data set is {}.'.format(newPreds.count()))\n",
    "print('The number of rows labeled high is {}.'.format(test.filter(newPreds['VETTING_LEVEL'] == 10).count()))\n",
    "print('The number of rows labeled medium is {}.'.format(test.filter(newPreds['VETTING_LEVEL'] == 20).count()))\n",
    "print('The number of rows labeled low is {}.'.format(test.filter(newPreds['VETTING_LEVEL'] == 30).count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, write the results back to dashDB.  This is done using a library called PixieDust, which can be used to do other visualizations and monitoring in notebooks.  We need to register a Scala bridge to do this.   \n",
    "\n",
    "Why? (JEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%scala cl=dialect global=true\n",
    "import org.apache.spark.sql.jdbc._\n",
    "import org.apache.spark.sql.types.{StringType, BooleanType, DataType}\n",
    "\n",
    "object dashDBCustomDialect extends JdbcDialect {\n",
    "    override def canHandle(url: String): Boolean = url.startsWith(\"jdbc:db2\")\n",
    "    override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {\n",
    "            case StringType => Option(JdbcType(\"VARCHAR(\" + maxStringColumnLength + \")\", java.sql.Types.VARCHAR))\n",
    "            case BooleanType => Option(JdbcType(\"CHAR(1)\", java.sql.Types.CHAR))\n",
    "            case _ => None\n",
    "    }\n",
    "}\n",
    "JdbcDialects.registerDialect(dashDBCustomDialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downselect all the values we need to join in our next lab to display the results, and write them to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valuesToWrite= newPreds.select(\"UUID\",  \"predCategory\")\n",
    "\n",
    "valuesToWrite.write.jdbc(credentials_1[\"jdbcurl\"], \"FEMALE_TRAFFICKING_ML_RESULTS\", properties = {\"user\" : credentials_1[\"username\"], \"password\" : credentials_1[\"password\"]}, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}